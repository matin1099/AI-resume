{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12291e8b-fa05-43c4-b1c8-fc8ee1434436",
   "metadata": {},
   "source": [
    "# Classification Approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebba46b-c316-44d0-a809-131fdb453b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (confusion_matrix, classification_report,\n",
    "ConfusionMatrixDisplay,PrecisionRecallDisplay,RocCurveDisplay)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Model(s)\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba915cf-2ad3-4a2b-9444-75d54398c9c5",
   "metadata": {},
   "source": [
    "## Work flow\n",
    "0. [ ] Clean and Engeering Data for X and y\n",
    "1. [ ] Split Data in Train/Test for X and y\n",
    "2. [ ] Scaler on Training X & X test\n",
    "3. [ ] Create Model(s)\n",
    "4. [ ] Create Pipeline and HyperParameters\n",
    "5. [ ] Fit/Train Model(s) on X Train\n",
    "6. [ ] Evaluate Model(s) on X test\n",
    "7. [ ] Adjust Param as Necessary\n",
    "8. [ ] Bonus: Save Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e942b-4aa1-4cce-a20f-d21389ca4b4d",
   "metadata": {},
   "source": [
    "### PreProcess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f27644-3869-4856-89d8-b8af31eeaf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wifi 1</th>\n",
       "      <th>wifi 2</th>\n",
       "      <th>wifi 3</th>\n",
       "      <th>wifi 4</th>\n",
       "      <th>wifi 5</th>\n",
       "      <th>wifi 6</th>\n",
       "      <th>wifi 7</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-64</td>\n",
       "      <td>-56</td>\n",
       "      <td>-61</td>\n",
       "      <td>-66</td>\n",
       "      <td>-71</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-68</td>\n",
       "      <td>-57</td>\n",
       "      <td>-61</td>\n",
       "      <td>-65</td>\n",
       "      <td>-71</td>\n",
       "      <td>-85</td>\n",
       "      <td>-85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-63</td>\n",
       "      <td>-60</td>\n",
       "      <td>-60</td>\n",
       "      <td>-67</td>\n",
       "      <td>-76</td>\n",
       "      <td>-85</td>\n",
       "      <td>-84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-61</td>\n",
       "      <td>-60</td>\n",
       "      <td>-68</td>\n",
       "      <td>-62</td>\n",
       "      <td>-77</td>\n",
       "      <td>-90</td>\n",
       "      <td>-80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-63</td>\n",
       "      <td>-65</td>\n",
       "      <td>-60</td>\n",
       "      <td>-63</td>\n",
       "      <td>-77</td>\n",
       "      <td>-81</td>\n",
       "      <td>-87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-59</td>\n",
       "      <td>-59</td>\n",
       "      <td>-48</td>\n",
       "      <td>-66</td>\n",
       "      <td>-50</td>\n",
       "      <td>-86</td>\n",
       "      <td>-94</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-59</td>\n",
       "      <td>-56</td>\n",
       "      <td>-50</td>\n",
       "      <td>-62</td>\n",
       "      <td>-47</td>\n",
       "      <td>-87</td>\n",
       "      <td>-90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-62</td>\n",
       "      <td>-59</td>\n",
       "      <td>-46</td>\n",
       "      <td>-65</td>\n",
       "      <td>-45</td>\n",
       "      <td>-87</td>\n",
       "      <td>-88</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-62</td>\n",
       "      <td>-58</td>\n",
       "      <td>-52</td>\n",
       "      <td>-61</td>\n",
       "      <td>-41</td>\n",
       "      <td>-90</td>\n",
       "      <td>-85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-59</td>\n",
       "      <td>-50</td>\n",
       "      <td>-45</td>\n",
       "      <td>-60</td>\n",
       "      <td>-45</td>\n",
       "      <td>-88</td>\n",
       "      <td>-87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wifi 1  wifi 2  wifi 3  wifi 4  wifi 5  wifi 6  wifi 7  room\n",
       "0        -64     -56     -61     -66     -71     -82     -81     1\n",
       "1        -68     -57     -61     -65     -71     -85     -85     1\n",
       "2        -63     -60     -60     -67     -76     -85     -84     1\n",
       "3        -61     -60     -68     -62     -77     -90     -80     1\n",
       "4        -63     -65     -60     -63     -77     -81     -87     1\n",
       "...      ...     ...     ...     ...     ...     ...     ...   ...\n",
       "1995     -59     -59     -48     -66     -50     -86     -94     4\n",
       "1996     -59     -56     -50     -62     -47     -87     -90     4\n",
       "1997     -62     -59     -46     -65     -45     -87     -88     4\n",
       "1998     -62     -58     -52     -61     -41     -90     -85     4\n",
       "1999     -59     -50     -45     -60     -45     -88     -87     4\n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['wifi 1', 'wifi 2', 'wifi 3', 'wifi 4', 'wifi 5', 'wifi 6', 'wifi 7', 'room']\n",
    "df= pd.read_csv('../wifi_localization.txt',names=names,sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de3f6d-8ba5-4f71-a346-cbe315f3b80d",
   "metadata": {},
   "source": [
    "#### Clean and Engeering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a11ff-f938-41f4-95d9-42ff0616a7ea",
   "metadata": {},
   "source": [
    "#### Split Data in Train/Test for X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc920b8-8109-4680-b3b0-3339df663543",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['room'],axis=1)\n",
    "y = df['room']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=1099, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046338d-f8fc-45ad-9881-036fa8a7e110",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d8ba2d-b4a5-409f-94d9-823b1ccb12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()\n",
    "\n",
    "grad = GradientBoostingClassifier()\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "pipe_grad = Pipeline([('poly', poly), ('grad', grad)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34299b34-b50f-4fc7-abc9-e06d39f47a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  This algorithm builds an additive model in a forward stage-wise fashion; it\n",
      " |  allows for the optimization of arbitrary differentiable loss functions. In\n",
      " |  each stage ``n_classes_`` regression trees are fit on the negative gradient\n",
      " |  of the loss function, e.g. binary or multiclass log loss. Binary\n",
      " |  classification is a special case where only a single regression tree is\n",
      " |  induced.\n",
      " |  \n",
      " |  :class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n",
      " |  variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'log_loss', 'deviance', 'exponential'}, default='log_loss'\n",
      " |      The loss function to be optimized. 'log_loss' refers to binomial and\n",
      " |      multinomial deviance, the same as used in logistic regression.\n",
      " |      It is a good choice for classification with probabilistic outputs.\n",
      " |      For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |      .. deprecated:: 1.1\n",
      " |          The loss 'deviance' was deprecated in v1.1 and will be removed in\n",
      " |          version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |      Values must be in the range `[0.0, inf)`.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |      Values must be in the range `[1, inf)`.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |      Values must be in the range `(0.0, 1.0]`.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      'friedman_mse' for the mean squared error with improvement score by\n",
      " |      Friedman, 'squared_error' for mean squared error. The default value of\n",
      " |      'friedman_mse' is generally the best as it can provide a better\n",
      " |      approximation in some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, values must be in the range `[2, inf)`.\n",
      " |      - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
      " |        will be `ceil(min_samples_split * n_samples)`.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, values must be in the range `[1, inf)`.\n",
      " |      - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
      " |        will be `ceil(min_samples_leaf * n_samples)`.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |      Values must be in the range `[0.0, 0.5]`.\n",
      " |  \n",
      " |  max_depth : int or None, default=3\n",
      " |      Maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |      If int, values must be in the range `[1, inf)`.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |      Values must be in the range `[0.0, inf)`.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random splitting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, values must be in the range `[1, inf)`.\n",
      " |      - If float, values must be in the range `(0.0, 1.0]` and the features\n",
      " |        considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |      Values must be in the range `[0, inf)`.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      Values must be in the range `[2, inf)`.\n",
      " |      If `None`, then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Values must be in the range `(0.0, 1.0)`.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |      Values must be in the range `[1, inf)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |      Values must be in the range `[0.0, inf)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
      " |      Values must be in the range `[0.0, inf)`.\n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |      .. deprecated:: 1.1\n",
      " |           Attribute `loss_` was deprecated in version 1.1 and will be\n",
      " |          removed in 1.3.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      " |      Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      " |      tree classifiers on various sub-samples of the dataset and uses\n",
      " |      averaging to improve the predictive accuracy and control over-fitting.\n",
      " |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      " |      on the original dataset and then fits additional copies of the\n",
      " |      classifier on the same dataset where the weights of incorrectly\n",
      " |      classified instances are adjusted such that subsequent classifiers\n",
      " |      focus more on difficult cases.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to fit a gradient boosting classifier with\n",
      " |  100 decision stumps as weak learners.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_hastie_10_2\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  \n",
      " |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      " |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      " |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      " |  \n",
      " |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      " |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.913...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape (n_samples,).\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  loss_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  base_estimator_\n",
      " |      Estimator used to grow the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516cd07b-4a45-4d15-ada5-943883e4a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyp_param value GRAD\n",
    "#poly\n",
    "degree = list(range(1,7,1))\n",
    "#grad\n",
    "loss_grad = ['log_loss', 'exponential']\n",
    "learning_rate_grad = list(np.logspace(-4,4,10))\n",
    "n_estimators_grad = [50, 100, 150, 200, 300 ]\n",
    "criterion_grad = ['friedman_mse', 'squared_error']\n",
    "max_depth_grad = [1,2,3,4,5,6,7]\n",
    "max_features_grad = [ 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d55e2c-1204-4702-9174-9a47c88bf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_hyp_param ={\n",
    "    #poly\n",
    "    'poly__degree': degree, \n",
    "    #grad\n",
    "    'grad__criterion': criterion_grad,\n",
    "    'grad__learning_rate': learning_rate_grad,\n",
    "    'grad__loss': loss_grad,\n",
    "    'grad__max_depth': max_depth_grad,\n",
    "    'grad__max_features': max_features_grad,\n",
    "    'grad__n_estimators': n_estimators_grad,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f05d9b5-a12b-49d9-96fb-97df8f02e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_grad = GridSearchCV(\n",
    "    estimator=pipe_grad,\n",
    "    param_grid=grad_hyp_param,\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ac3c2-c2a2-4052-a880-b84a372e0d27",
   "metadata": {},
   "source": [
    "#### Create Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db289a-97e7-4348-ae05-e19899cf9a88",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7627e83c-e2bd-4092-a8f4-48639ecdf941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16800 candidates, totalling 84000 fits\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   3.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   2.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   2.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   4.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   2.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=   4.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=   4.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=   4.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=   4.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=   4.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=   5.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=   6.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=   6.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=   6.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=   5.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   3.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   3.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   2.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=1, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   3.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   4.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   4.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   4.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   4.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   4.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   6.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   6.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   6.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   6.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   9.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   9.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   9.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   9.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=   9.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   4.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   3.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   4.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  14.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  14.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  14.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  14.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  14.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=5; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=100, poly__degree=6; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=4; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=5; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=150, poly__degree=6; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=3; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=4; total time=   3.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   3.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   3.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   3.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=5; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   3.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   3.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=200, poly__degree=6; total time=   3.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=1; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   2.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   2.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=2; total time=   2.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   3.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=3; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   4.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   4.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   4.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   4.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   5.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   5.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   5.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=5; total time=   4.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   5.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   5.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   5.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   6.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=2, grad__max_features=log2, grad__n_estimators=300, poly__degree=6; total time=   5.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=1; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=2; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=4; total time=   1.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=5; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=50, poly__degree=6; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   2.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=3; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   3.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   3.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   3.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   3.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=4; total time=   3.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=5; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   5.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   5.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   5.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   5.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=100, poly__degree=6; total time=   5.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=2; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   2.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=3; total time=   1.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=4; total time=   3.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=5; total time=   5.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=150, poly__degree=6; total time=   7.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=1; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=2; total time=   1.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=3; total time=   2.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=4; total time=   4.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   7.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   7.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   7.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   7.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=5; total time=   7.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=  10.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=  10.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=  10.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=  10.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=200, poly__degree=6; total time=  10.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=1; total time=   1.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=2; total time=   2.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   4.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   3.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   3.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=3; total time=   3.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=4; total time=   6.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=5; total time=  10.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  15.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  15.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  15.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  15.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=sqrt, grad__n_estimators=300, poly__degree=6; total time=  15.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.3s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=1; total time=   0.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=2; total time=   0.4s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=3; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=4; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.8s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=5; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   1.1s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=50, poly__degree=6; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.6s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.5s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=1; total time=   0.7s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   0.9s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=2; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.2s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=3; total time=   1.0s\n",
      "[CV] END grad__criterion=friedman_mse, grad__learning_rate=0.0001, grad__loss=log_loss, grad__max_depth=3, grad__max_features=log2, grad__n_estimators=100, poly__degree=4; total time=   1.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m full_model_grad\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[1;32m    539\u001b[0m     X,\n\u001b[1;32m    540\u001b[0m     y,\n\u001b[1;32m    541\u001b[0m     raw_predictions,\n\u001b[1;32m    542\u001b[0m     sample_weight,\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[1;32m    544\u001b[0m     X_val,\n\u001b[1;32m    545\u001b[0m     y_val,\n\u001b[1;32m    546\u001b[0m     sample_weight_val,\n\u001b[1;32m    547\u001b[0m     begin_at_stage,\n\u001b[1;32m    548\u001b[0m     monitor,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[1;32m    616\u001b[0m     i,\n\u001b[1;32m    617\u001b[0m     X,\n\u001b[1;32m    618\u001b[0m     y,\n\u001b[1;32m    619\u001b[0m     raw_predictions,\n\u001b[1;32m    620\u001b[0m     sample_weight,\n\u001b[1;32m    621\u001b[0m     sample_mask,\n\u001b[1;32m    622\u001b[0m     random_state,\n\u001b[1;32m    623\u001b[0m     X_csc,\n\u001b[1;32m    624\u001b[0m     X_csr,\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:260\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    257\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    262\u001b[0m     X,\n\u001b[1;32m    263\u001b[0m     y,\n\u001b[1;32m    264\u001b[0m     residual,\n\u001b[1;32m    265\u001b[0m     raw_predictions,\n\u001b[1;32m    266\u001b[0m     sample_weight,\n\u001b[1;32m    267\u001b[0m     sample_mask,\n\u001b[1;32m    268\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# add tree to ensemble\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[i, k] \u001b[38;5;241m=\u001b[39m tree\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb_losses.py:121\u001b[0m, in \u001b[0;36mLossFunction.update_terminal_regions\u001b[0;34m(self, tree, X, y, residual, raw_predictions, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# update each leaf (= perform line search)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m leaf \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(tree\u001b[38;5;241m.\u001b[39mchildren_left \u001b[38;5;241m==\u001b[39m TREE_LEAF)[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_terminal_region(\n\u001b[1;32m    122\u001b[0m         tree,\n\u001b[1;32m    123\u001b[0m         masked_terminal_regions,\n\u001b[1;32m    124\u001b[0m         leaf,\n\u001b[1;32m    125\u001b[0m         X,\n\u001b[1;32m    126\u001b[0m         y,\n\u001b[1;32m    127\u001b[0m         residual,\n\u001b[1;32m    128\u001b[0m         raw_predictions[:, k],\n\u001b[1;32m    129\u001b[0m         sample_weight,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# update predictions (both in-bag and out-of-bag)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m raw_predictions[:, k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m tree\u001b[38;5;241m.\u001b[39mvalue[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m    134\u001b[0m     terminal_regions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    135\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sklearn/ensemble/_gb_losses.py:839\u001b[0m, in \u001b[0;36mMultinomialDeviance._update_terminal_region\u001b[0;34m(self, tree, terminal_regions, leaf, X, y, residual, raw_predictions, sample_weight)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_terminal_region\u001b[39m(\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    829\u001b[0m     tree,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    836\u001b[0m     sample_weight,\n\u001b[1;32m    837\u001b[0m ):\n\u001b[1;32m    838\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make a single Newton-Raphson step.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m     terminal_region \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(terminal_regions \u001b[38;5;241m==\u001b[39m leaf)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    840\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mtake(terminal_region, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    841\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtake(terminal_region, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_model_grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc73c9-09fd-4f02-921f-b7d33127da81",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c60c9-cef4-4ebf-9127-ee39280a5292",
   "metadata": {},
   "source": [
    "#### Test On data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461f140-4c7e-4cd4-b831-ad50af73132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grad = full_model_grad(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87eef61-0618-4167-8f5b-509456506a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_test,y_pred=y_pred_grad),\n",
    "    display_labels=full_model_grad.classes_\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58d5a6-cca3-46fb-b1ee-54f5e76417e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test,y_pred=y_pred_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b96fdc-ae0e-4197-8232-d9da2e4687a4",
   "metadata": {},
   "source": [
    "### Final Model(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4fe54-02b3-484f-b998-b033deed4205",
   "metadata": {},
   "source": [
    "#### Train on all Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986536d-099f-4f96-b919-1b0a255728be",
   "metadata": {},
   "source": [
    "##### no need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e33b81-ae1c-437d-98d8-12df18db766a",
   "metadata": {},
   "source": [
    "#### Save with joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94271c64-de9f-4cd9-9a8e-39f68745d794",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(value=full_model,filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d85c1-9440-452a-866a-b8be884765ea",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a066bb27-ecc4-46ed-8744-6da30ed8a4fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostClassifier in module sklearn.ensemble._weight_boosting:\n",
      "\n",
      "class AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      " |  AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')\n",
      " |  \n",
      " |  An AdaBoost classifier.\n",
      " |  \n",
      " |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      " |  classifier on the original dataset and then fits additional copies of the\n",
      " |  classifier on the same dataset but where the weights of incorrectly\n",
      " |  classified instances are adjusted such that subsequent classifiers focus\n",
      " |  more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : object, default=None\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required, as well as proper\n",
      " |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      " |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      initialized with `max_depth=1`.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator` was renamed to `estimator`.\n",
      " |  \n",
      " |  n_estimators : int, default=50\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |      Values must be in the range `[1, inf)`.\n",
      " |  \n",
      " |  learning_rate : float, default=1.0\n",
      " |      Weight applied to each classifier at each boosting iteration. A higher\n",
      " |      learning rate increases the contribution of each classifier. There is\n",
      " |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      " |      Values must be in the range `(0.0, inf)`.\n",
      " |  \n",
      " |  algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n",
      " |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      " |      ``estimator`` must support calculation of class probabilities.\n",
      " |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      " |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      " |      achieving a lower test error with fewer boosting iterations.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given at each `estimator` at each\n",
      " |      boosting iteration.\n",
      " |      Thus, it is only used when `estimator` exposes a `random_state`.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  base_estimator : object, default=None\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required, as well as proper\n",
      " |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      " |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      initialized with `max_depth=1`.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator` instead.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator_` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator_` instead.\n",
      " |  \n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  estimator_weights_ : ndarray of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : ndarray of floats\n",
      " |      Classification error for each estimator in the boosted\n",
      " |      ensemble.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances if supported by the\n",
      " |      ``estimator`` (when based on decision trees).\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n",
      " |      regressor on the original dataset and then fits additional copies of\n",
      " |      the regressor on the same dataset but where the weights of instances\n",
      " |      are adjusted according to the error of the current prediction.\n",
      " |  \n",
      " |  GradientBoostingClassifier : GB builds an additive model in a forward\n",
      " |      stage-wise fashion. Regression trees are fit on the negative gradient\n",
      " |      of the binomial or multinomial deviance loss function. Binary\n",
      " |      classification is a special case where only a single regression tree is\n",
      " |      induced.\n",
      " |  \n",
      " |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n",
      " |      method used for classification.\n",
      " |      Creates a model that predicts the value of a target variable by\n",
      " |      learning simple decision rules inferred from the data features.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.983...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseWeightBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape of (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict classes for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class log-probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each boosting iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each boosting iteration.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble predicted class probabilities\n",
      " |      after each iteration of boosting and therefore allows monitoring, such\n",
      " |      as to determine the predicted class probabilities on a test set after\n",
      " |      each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      p : generator of ndarray of shape (n_samples,)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted classifier/regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          1 / n_samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The feature importances.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  base_estimator_\n",
      " |      Estimator used to grow the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AdaBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c355d60a-f75d-4b93-84d4-c124ee30de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ada = Pipeline([ ('ada', ada)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35402aa1-f525-4379-a0af-dc1c4683063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyp_param value ADA\n",
    "#poly\n",
    "degree = list(range(1,4,1))\n",
    "#grad\n",
    "n_estimators_ada = [25, 50, 100, 150,  300 ]\n",
    "algorithm = ['SAMME', 'SAMME.R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c52bd4-889a-42f1-99e9-5f4863eaf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_hyp_param ={\n",
    "    #poly\n",
    "    #'poly__degree': degree, \n",
    "    #grad\n",
    "    'ada__n_estimators' : n_estimators_ada,\n",
    "    'ada__algorithm' : algorithm\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c211517-3d9d-452c-a681-015ec80804f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_ada = GridSearchCV(\n",
    "    estimator=pipe_ada,\n",
    "    param_grid=ada_hyp_param,\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dac83-0f0f-4f35-8d4c-9c179ca63df6",
   "metadata": {},
   "source": [
    "#### Create Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add990c8-fe94-4613-a1ce-f62f8b6a28d6",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7c2563c-6076-4dcb-a34a-36afead415f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(n_estimators=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(n_estimators=150)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full_model_ada.fit(X_train, y_train)\n",
    "temp = AdaBoostClassifier(n_estimators=150)\n",
    "temp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3542bf-84f8-4b61-a42d-3c9f6b4e4314",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02861e02-f542-409b-b0d4-76a53e526ce3",
   "metadata": {},
   "source": [
    "#### Test On data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09c2ed2a-119f-4605-999c-f1ce392ef2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_ada = full_model_ada.pr(X_test)\n",
    "\n",
    "temp_y= temp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "212ad953-c3f9-4bd8-a576-a49e13e7feec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff556d6c110>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_true=y_test,y_pred=temp_y),\n",
    "    display_labels=temp.classes_\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe7c59f0-8c46-4079-b0eb-16ee2f122142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.99      0.98       165\n",
      "           2       0.94      0.92      0.93       165\n",
      "           3       0.85      0.93      0.89       165\n",
      "           4       1.00      0.90      0.95       165\n",
      "\n",
      "    accuracy                           0.94       660\n",
      "   macro avg       0.94      0.94      0.94       660\n",
      "weighted avg       0.94      0.94      0.94       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test,y_pred=temp_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e8e27-7f05-4981-82f6-f6b12bab34a7",
   "metadata": {},
   "source": [
    "### Final Model(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abd814e2-be54-4148-ba3e-1252749f9452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': 'deprecated',\n",
       " 'estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d53ca7-1713-4b76-aeb5-0f4da591a03c",
   "metadata": {},
   "source": [
    "#### Train on all Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc2438-0ece-4654-a2cf-e60f013f85bb",
   "metadata": {},
   "source": [
    "##### no need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7a455-bbd9-477f-b91d-37e5257cd381",
   "metadata": {},
   "source": [
    "#### Save with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3624ba21-fa21-4386-81cb-6e1cd42f383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(value=full_model,filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
